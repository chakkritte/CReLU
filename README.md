# CReLU (Concatenated Rectified Linear Units)  with Caffe
CReLU (Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units)

By Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee


Paper : https://arxiv.org/pdf/1603.05201v2.pdf

If you use these models in your research, please cite:

    @article{shang2016understanding,
      title={Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units},
      author={Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
      journal={arXiv preprint arXiv:1603.05201},
      year={2016}
    }

### Results

1-crop validation error on ImageNet (center 224x224 crop from resized image with shorter side=256):

	model|top-1|top-5
	:---:|:---:|:---:
	AlexNet|42.8%|19.7%  
	SqueezeNet|42.5%|19.7%
	CReLU (all) in paper|40.93%|19.39% 
	CReLU (conv1,4,7) in paper|40.45 %|18.58% 
	**CReLU (conv1â€“4) in paper**|**39.82**%|**18.28**%



### Donate

![Build Status](https://raw.githubusercontent.com/chakkritte/NU-InNet/master/images/pic.png)

 (Bitcoin Address) : 3DermWyouJQ5MwoNvEtv2SgnvFRMTDBudv
